{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 3 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "Appended 2 days of data to /Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas/combined_internet_traffic_data_dallas.parquet\n",
      "All data processed and written incrementally.\n",
      "Parquet writer closed.\n",
      "   export_sysid                  t_first                   t_last  proto  \\\n",
      "0            12  2021-10-18T00:00:00.768  2021-10-18T00:00:00.768      6   \n",
      "1             6  2021-10-18T00:03:36.576  2021-10-18T00:03:36.576      6   \n",
      "2            12  2021-10-18T00:01:48.800  2021-10-18T00:01:48.800      6   \n",
      "3            12  2021-10-17T23:59:58.464  2021-10-18T00:00:24.320      6   \n",
      "4            11   2021-10-18T00:03:05.88   2021-10-18T00:03:05.88     17   \n",
      "\n",
      "       src4_addr     dst4_addr  src_port  dst_port  src_tos  dst_tos  ...  \\\n",
      "0    128.227.0.0    52.217.0.0     55818       443        0        0  ...   \n",
      "1   52.223.240.0  167.96.152.0       443     56751        0        0  ...   \n",
      "2  159.178.216.0    3.91.112.0     51638       443        0        0  ...   \n",
      "3   66.218.144.0  44.237.152.0     46759       443        0        0  ...   \n",
      "4    69.88.184.0   54.87.136.0      9908     33435        0        0  ...   \n",
      "\n",
      "   output_snmp      src_as  dst_as  src_mask  dst_mask    ip4_next_hop  \\\n",
      "0          508        6356   16509        16        24  162.252.70.143   \n",
      "1          511  4294967295    2055         0        16  162.252.70.135   \n",
      "2          508        6356   14618        16        12  162.252.70.143   \n",
      "3          508       36695   16509        20        11  162.252.70.143   \n",
      "4          508       17202   14618        19        16  162.252.70.143   \n",
      "\n",
      "       ip4_router src6_addr dst6_addr ip6_next_hop  \n",
      "0  162.252.70.246      None      None         None  \n",
      "1  162.252.70.246      None      None         None  \n",
      "2  162.252.70.246      None      None         None  \n",
      "3  162.252.70.246      None      None         None  \n",
      "4  162.252.70.246      None      None         None  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Base path where your files are located\n",
    "# base_path = '/Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Dallas'\n",
    "base_path = '/Users/maushariff/Downloads/Local_Disk_E/Byrav/Internet2_Data/Data/Atla'\n",
    "\n",
    "# List all files in the directory\n",
    "file_names = os.listdir(base_path)\n",
    "\n",
    "# Generate full paths to the files\n",
    "file_paths = [os.path.join(base_path, file_name) for file_name in file_names]\n",
    "\n",
    "# Set the output path for the parquet file\n",
    "# output_parquet_path = os.path.join(base_path, 'combined_internet_traffic_data_atla.parquet')\n",
    "output_parquet_path = os.path.join(base_path, 'combined_internet_traffic_data_dallas.parquet')\n",
    "\n",
    "\n",
    "# Set the chunk size (i.e., how many files to process at a time â€” here we're taking 3 days worth of files)\n",
    "days_to_process = 3\n",
    "\n",
    "# Initialize Parquet writer\n",
    "parquet_writer = None\n",
    "\n",
    "# Function to process and append data to the existing parquet file\n",
    "def process_and_append_data(file_paths, parquet_path, days_to_process=3):\n",
    "    global parquet_writer\n",
    "\n",
    "    for i in range(0, len(file_paths), days_to_process):\n",
    "        # Process the next batch of files (e.g., 3 days of data)\n",
    "        batch_files = file_paths[i:i + days_to_process]\n",
    "        df_list = []  # List to hold DataFrames from files\n",
    "\n",
    "        for file_path in batch_files:\n",
    "            data_list = []\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        # Append each valid JSON object to the list\n",
    "                        data_list.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error decoding JSON on this line: {line}\")\n",
    "                        print(e)\n",
    "\n",
    "            # Convert the data to a DataFrame and append to the list\n",
    "            df = pd.DataFrame(data_list)\n",
    "            df_list.append(df)\n",
    "\n",
    "        # Concatenate the DataFrames from the current batch\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Convert DataFrame to a PyArrow Table\n",
    "        table = pa.Table.from_pandas(combined_df)\n",
    "\n",
    "        # Append or create the Parquet file\n",
    "        if parquet_writer is None:\n",
    "            parquet_writer = pq.ParquetWriter(parquet_path, table.schema, compression='snappy')\n",
    "\n",
    "        parquet_writer.write_table(table)\n",
    "        print(f\"Appended {len(batch_files)} days of data to {parquet_path}\")\n",
    "\n",
    "    print(\"All data processed and written incrementally.\")\n",
    "\n",
    "# Close the Parquet writer once the processing is done\n",
    "def close_parquet_writer():\n",
    "    global parquet_writer\n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "        print(\"Parquet writer closed.\")\n",
    "\n",
    "# Incrementally process data in 3-day batches and append to the Parquet file\n",
    "process_and_append_data(file_paths, output_parquet_path, days_to_process=3)\n",
    "\n",
    "# Close the Parquet writer when done\n",
    "close_parquet_writer()\n",
    "\n",
    "# Now you can load and verify the combined data\n",
    "df_combined_parquet = pd.read_parquet(output_parquet_path)\n",
    "print(df_combined_parquet.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
